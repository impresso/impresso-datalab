---
title: Named Entity Processing with Impresso Models
githubUrl: https://github.com/impresso/impresso-datalab-notebooks/blob/main/annotate/NE-processing_ImpressoHF.ipynb
authors:
  - impresso-team
excerpt: Trained on the [HIPE
  2020](https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-hipe2020.md)
  dataset, the Impresso models recognize both coarse and fine-grained named
  entities, linking mentions to knowledge bases when possible.
sha: 44a3c9f14c74807de3722878701d97ed71fa3e05
date: 2024-10-25T14:18:01Z
googleColabUrl: https://colab.research.google.com/github/impresso/impresso-datalab-notebooks/blob/main/annotate/NE-processing_ImpressoHF.ipynb
seealso:
  - ne-processing-with-impresso-api
links:
  - href: https://huggingface.co/impresso-project/
    label: Hugging Face
  - href: https://github.com/hipe-eval/HIPE-2022-data
    label: HIPE-2022 dataset
  - href: https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-hipe2020.md
    label: HIPE typology
  - href: https://huggingface.co/spaces/impresso-project/multilingual-named-entity-recognition
    label: NER space
  - href: https://huggingface.co/spaces/impresso-project/multilingual-entity-linking
    label: EL space
  - href: https://huggingface.co/join
    label: Hugging Face website
  - href: https://huggingface.co/docs/huggingface_hub/v0.20.2/en/quick-start#environment-variable
    label: official documentation
---

{/* cell:0 cell_type:markdown */}
## What is this notebook about?

This notebook demonstrates how to use Impresso models for named entity recognition (NER) and entity linking (EL).

NER detects and classifies entities such as persons, locations, and organizations in text, while EL connects recognized entities to unique identifiers in a knowledge base, like Wikipedia or its data counterpart, Wikidata.

In this notebook, both NER and EL are performed using models trained by Impresso and hosted on [Hugging Face](https://huggingface.co/impresso-project/) (thus the 'HF' suffix in the notebook name):

- The **Impresso NER model** is a Transformer model trained on the Impresso HIPE-2020 portion of the [HIPE-2022 dataset](https://github.com/hipe-eval/HIPE-2022-data). It recognizes entity types such as person, location, and organization while supporting the complete [HIPE typology](https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-hipe2020.md), including coarse and fine-grained entity types as well as components like names, titles, and roles.
  
- The **Impresso NEL model** links detected entities to unique identifiers in Wikipedia and Wikidata or assigns a 'NIL' label (indicating "not in list" in NLP) if no reference is found.

Both models can also be tested interactively in Hugging Face spaces: the [NER space](https://huggingface.co/spaces/impresso-project/multilingual-named-entity-recognition) and the [EL space](https://huggingface.co/spaces/impresso-project/multilingual-entity-linking).

## What will you learn in this notebook?

By the end of this notebook, you will know how to:
- Install the necessary libraries to run the models
- Load the models and modules from Hugging Face
- Perform NER on a text input
- Perform EL on the NER output

**Warning**:
To use this notebook, you may need to set the `HF_TOKEN` environment variable in the `.env` file (refer to `.env.example`). You can obtain a token by signing up on the [Hugging Face website](https://huggingface.co/join) and find additional information in the [official documentation](https://huggingface.co/docs/huggingface_hub/v0.20.2/en/quick-start#environment-variable). If you do not want to register an account on HF, simply select Cancel when prompted for a Hugging Face token â€” no token is needed for this notebook.

{/* cell:1 cell_type:markdown */}
## Prerequisites
First, we install and download necessary libraries:

- **torch**: PyTorch is a popular open-source library for deep learning that provides tools for tensor computation, GPU acceleration, and building neural networks.
- **protobuf**: Protobuf, short for 'Protocol Buffers', is a library developed by Google for serializing structured data in a fast and efficient way, ideal for communication between services. 
- **sentencepiece**: SentencePiece is a text processing library used primarily for tokenization, especially for languages with complex scripts. It supports subword tokenization, which is key for training language models that need flexible token units (e.g., parts of words). BERT and transformers in general often use SentencePiece for multilingual support.
- **transformers**:  Developed by Hugging Face, this library offers many functionalities to support the development of NLP deep learning models. It provides pre-trained models for various tasks, supports architectures like BERT, GPT, T5, and others for model developement and manipulation, offers useful pipelines and easily integrates with PyTorch. The models developed by Impresso are BERT-based-
**nltk**: The Natural Language Toolkit is a library for NLP in Python that offers tools for text processing tasks like tokenization, stemming, lemmatization, and parsing, as well as datasets for linguistic research and training. 

Libraries can be installed from the notebook, or within your environment:

```bash
pip install torch protobuf sentencepiece transformers nltk
```

{/* cell:2 cell_type:code */}
```python
!pip install torch
!pip install protobuf
!pip install sentencepiece
!pip install transformers
!pip install nltk
```

{/* cell:3 cell_type:markdown */}
## Entity Recognition

{/* cell:4 cell_type:code */}
```python
# Import necessary Python modules from the Transformers library
from transformers import AutoModelForTokenClassification, AutoTokenizer
from transformers import pipeline
```

{/* cell:5 cell_type:markdown */}
For NER, we use the Impresso NER model named 'ner-stacked-bert-multilingual' and published on Hugging Face: https://huggingface.co/impresso-project/ner-stacked-bert-multilingual.

{/* cell:6 cell_type:code */}
```python
# We set the model_name variable to our chosen model, enabling us to load it and use it for token classification and NER
MODEL_NAME = "impresso-project/ner-stacked-bert-multilingual"

# Load the tokenizer corresponding to the model
ner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

{/* cell:7 cell_type:markdown */}
It is necessary to create a pipeline for our task (`generic-ner`), using the loaded model and tokenizer. This pipeline handles multiple tasks under the hood.


{/* cell:8 cell_type:code */}
```python
ner_pipeline = pipeline("generic-ner", model=MODEL_NAME, 
                        tokenizer=ner_tokenizer, 
                        trust_remote_code=True,
                        device='cpu')
```

{/* cell:9 cell_type:code */}
```python
# We define some test input
sentence = """In the year 1789, King Louis XVI, ruler of France, convened the Estates-General at the Palace of Versailles, 
                where Marie Antoinette, the Queen of France, alongside Maximilien Robespierre, a leading member of the National Assembly, 
                debated with Jean-Jacques Rousseau, the famous philosopher, and Charles de Talleyrand, the Bishop of Autun, 
                regarding the future of the French monarchy. At the same time, across the Atlantic in Philadelphia, 
                George Washington, the first President of the United States, and Thomas Jefferson, the nation's Secretary of State, 
                were drafting policies for the newly established American government following the signing of the Constitution."""

print(sentence)
```

{/* cell:10 cell_type:code */}
```python
# A function that formats and displays the model output in a readable structure
def print_nicely(data):
    for idx, entry in enumerate(data):
        for key, value in entry.items():
            print(f"  {key.capitalize()}: {value}")
        print()  # Blank line between entries
        
```

{/* cell:11 cell_type:markdown */}
We apply the pipeline on the input and print nicely the output

{/* cell:12 cell_type:code */}
```python
# Recognize stacked entities for each sentence
entities = ner_pipeline(sentence)

# Extract coarse and fine entities
print_nicely(entities)
```

{/* cell:13 cell_type:markdown */}
## Entity Linking

{/* cell:14 cell_type:markdown */}
With the EL model, we can link the previously recognised entity mentions to unique referents in Wikipedia and Wikidata.

We use the Impresso model named 'nel-mgenre-multilingual' and published on Hugging Face: https://huggingface.co/impresso-project/nel-mgenre-multilingual.

{/* cell:15 cell_type:code */}
```python
# Import the necessary modules from the transformers library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import pipeline

NEL_MODEL_NAME = "impresso-project/nel-mgenre-multilingual"

# Load the tokenizer and model from the specified pre-trained model name
# The model used here is "https://huggingface.co/impresso-project/nel-mgenre-multilingual"
nel_tokenizer = AutoTokenizer.from_pretrained("impresso-project/nel-mgenre-multilingual")
```

{/* cell:16 cell_type:code */}
```python
nel_pipeline = pipeline("generic-nel", model=NEL_MODEL_NAME, 
                        tokenizer=nel_tokenizer, 
                        trust_remote_code=True,
                        device='cpu')
```

{/* cell:17 cell_type:markdown */}
Our entity linker requires a specific format to correctly identify the entity that needs to be linked, as follows:

```
The event was held at the [START] Palace of Versailles [END], a symbol of French monarchy.
```

Assuming that the string "Palace of Versailles" was previously detected by an NER tool, we need to surround it with the `[START]` and `[END]` markers. 

The EL pipeline processes only one entity per input text. Therefore, for multiple entities within the same input, we must create separate inputs for each entity. For instance:

```
The event was held at the [START] Palace of Versailles [END], a symbol of French monarchy.
The event was held at the Palace of Versailles, a symbol of [START] French monarchy [END].
```

Let's take this example:

{/* cell:18 cell_type:code */}
```python
simple_sentence = "The event was held at the [START] Palace of Versailles [END], a symbol of French monarchy."

linked_entity = nel_pipeline(simple_sentence)

print_nicely(linked_entity)
```

{/* cell:19 cell_type:markdown */}
It _could_ work without the special markers and texts mentioning only one entity, but we do not recommend it.

{/* cell:20 cell_type:code */}
```python
simple_sentence = "The event was held at the Palace of Versailles, a symbol of French monarchy."

linked_entity = nel_pipeline(simple_sentence)

print_nicely(linked_entity)
```

{/* cell:21 cell_type:markdown */}
By using our NER tool, we can automatically generate sentences with entity markers and subsequently link each entity:

{/* cell:22 cell_type:code */}
```python
# Run the NER pipeline on the input sentence and store the results
entities = ner_pipeline(sentence)

print(f'{len(entities)} entities were detected.')

# List to keep track of already processed words to avoid duplicate tagging
already_done = []

# Process each entity for linking
for entity in entities:
    if entity['surface'] not in already_done:
        # Tag the entity in the text

        language = 'en'
        tokens = sentence.split(' ')
        start, end = (
            entity["index"][0],
            entity["index"][1],
        )

        context_start = max(0, start - 10)
        context_end = min(len(tokens), end + 11)

        nel_sentence = (
            " ".join(tokens[context_start:start])
            + " [START] "
            + entity['surface']
            + " [END] "
            + " ".join(tokens[end + 1 : context_end])
        )

        linked_entities = nel_pipeline(nel_sentence)
        print(nel_sentence)
        print_nicely(linked_entities)
```

{/* cell:23 cell_type:markdown */}
## Looking up entities in the Impresso Corpus

Are the previously recognised entities present in the Impresso Corpus? Let's explore using the Impresso API and Python Library:

{/* cell:24 cell_type:code */}
```python

```

{/* cell:25 cell_type:code */}
```python

```

{/* cell:26 cell_type:code */}
```python

```

{/* cell:27 cell_type:code */}
```python

```
